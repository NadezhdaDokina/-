# 1. Полное задание из методички с вариантом
### Задание 3: Time Series Prediction (Прогнозирование временных рядов) 
### Задача: создать LSTM сеть для прогнозирования цен акций.

### Требования: 
- Использовать скользящее окно для подготовки данных
- Архитектура: LSTM(50) → Dense(25) → Dense(1)
- Нормализация данных

### Что нужно дополнить: 
1. Реализацию LSTM ячейки 
2. Создание скользящих окон 
3. Нормализацию и денормализацию 
4. Визуализацию предсказаний

# Алгоритм и архитектура работы НС по блокам
Пошаговое описание алгоритма работы нейросети(НС)

### 1.Подготовка данных.
Собирается исторический ряд цен (обычно цена закрытия за каждый день).
Ряд нормализуется методом Min‑Max, чтобы все значения лежали в диапазоне 0–1 и сеть стабильнее училась.
Из нормализованных данных формируются скользящие окна длиной window_size:
-вход сети: последовательность из window_size прошлых значений [t - window_size,...,t-1]
-целевое значение: цена в момент 

### 2.Прямой проход (предсказание)
-Каждое окно подаётся в LSTM‑слой по одному шагу времени: на каждом шаге LSTM получает очередное значение и обновляет своё скрытое состояние и память.​
-После последнего шага LSTM выдаёт вектор признаков фиксированной длины 50 (последнее скрытое состояние).​
-Этот вектор проходит через полносвязный слой Dense(25), который выделяет более компактные признаки, а затем через выходной слой Dense(1), формирующий одно число — прогноз будущей цены в нормализованном масштабе.

### 3.Расчёт ошибки
-Прогноз (нормализованное число) сравнивается с истинным нормализованным значением для того же момента времени.​
-Вычисляется функция потерь — среднеквадратичная ошибка (MSE): средний квадрат разности между предсказанными и истинными значениями по мини‑батчу.

### 4.Обратное распространение и обновление весов
-На основе MSE вычисляются градиенты функции потерь по всем весам LSTM‑слоя и полносвязных слоёв.
-Градиенты распространяются «назад во времени» через все шаги LSTM (backpropagation through time), что позволяет скорректировать веса с учётом всей последовательности.
-Оптимизатор Adam обновляет веса сети: уменьшает ошибки для текущего батча, делая будущие прогнозы ближе к реальным значениям.

### 5.Обучение по эпохам
-Вся обучающая выборка окон разбивается на батчи; для каждого батча выполняются прямой проход, расчёт ошибки и обновление весов.
-После прохода по всем батчам считается средней loss и mse на обучении, а затем аналогично считаются loss и mse на отложенной валидационной части данных ( без обновления весов).
-Эти значения сохраняются в истории обучения и по ним строятся два графика:
  функция потерь обучения и валидации по эпохам;
  «точность» обучения и валидации, где в качестве метрики используется MSE.

# Архитектура
Класс TimeSeriesPredictor реализует архитектуру нейросети для прогноза временного ряда цен акций на основе LSTM.

Основные компоненты и методы:
- `__init__(...)`:  
  Инициализирует размер скользящего окна, создаёт модель LSTM‑архитектуры и объект нормализации данных (MinMaxScaler), а также контейнер для истории обучения.

- `_build_model(...)`:  
  Строит архитектуру нейросети:
  - вход: последовательность длиной `window_size` с одним признаком на шаг;  
  - слой `LSTM(50)`: извлекает временные зависимости и формирует скрытое представление последовательности;  
  - слой `Dense(25)`: сжимает и перерабатывает признаки, выделенные LSTM;  
  - выходной слой `Dense(1)`: выдаёт прогноз одного числового значения (следующая цена);  
  - компиляция модели с оптимизатором Adam, функцией потерь MSE и метрикой MSE.

- `prepare_data(data)`:  
  Преобразует одномерный временной ряд в обучающие пары «окно–цель»:
  - входы `X`: скользящие окна длиной `window_size`;  
  - выходы `y`: значение ряда сразу после окна.  
  Формирует тензор нужной формы для подачи в LSTM.

- `normalize(data)` / `inverse_transform(data)`:  
  Выполняют нормализацию временного ряда и обратное преобразование с помощью MinMaxScaler, чтобы:
  - на этапе обучения сеть работала с масштабированными данными;  
  - на этапе инференса переводить предсказания обратно в реальные цены.

- `train(X_train, y_train, ...)`:  
  Запускает процесс обучения:
  - выполняет многократные эпохи прямого и обратного прохода по обучающей выборке;  
  - считает MSE‑ошибку и метрику на обучении и валидации;  
  - сохраняет историю обучения (loss и mse по эпохам) в `self.history` для последующего анализа.

- `predict(last_window)`:  
  Реализует прямой проход модели для одного последнего окна:
  - принимает последовательность последних `window_size` значений (в нормализованном виде);  
  - прогоняет её через LSTM и полносвязные слои;  
  - выполняет денормализацию результата и возвращает прогноз следующей цены.

- `plot_loss_and_accuracy()`:  
  Визуализирует динамику обучения:
  - строит график функции потерь (train/val loss) по эпохам;  
  - строит график метрики MSE (train/val mse) по эпохам как «график точности».

Вспомогательные элементы архитектуры

В рамках этой архитектуры роль «примитивных блоков» играют стандартные слои и объекты:

- `LSTM`: базовый рекуррентный блок, запоминающий долгосрочные зависимости во временном ряду.  
- `Dense`: простые полносвязные слои для преобразования скрытых признаков.  
- `MinMaxScaler`: модуль предобработки данных, обеспечивающий нормализацию и обратную денормализацию.  
- Механизм истории обучения (`History`) из Keras: хранит значения функции потерь и метрик, используемые методами визуализации.

# Ответ на контрольный вопрос
### Что такое теорема Ардена и для чего она используется? 
Теорема Ардена является важным инструментом в теории формальных языков и автоматах, используемым для решения уравнений регулярных выражений. Она позволяет находить решение рекурсивных уравнений, возникающих при описании поведения конечных автоматов.
## Формулировка

- Рассматривается уравнение над регулярными выражениями вида  R = Q + RP, где R, Q, P- регулярные выражения над одним и тем же алфавитом, а P не содержит пустой строки.  
- Теорема Ардена утверждает, что такое уравнение имеет единственное решение  R = QP^*.  
То есть выражение Rможно переписать как «Q, за которым следует любое количество (в том числе нуль) повторений P».

## Для чего используется

- Для вычисления регулярного выражения по заданному конечному автомату: по вершинам автомата составляют систему уравнений вида R_i = Q_i + sum R_j P_ji и последовательно решают её, применяя теорему Ардена.  
- Для доказательства свойств регулярных языков и преобразования между разными представлениями (автомат ↔ регулярное выражение), поскольку позволяет строго вывести вид языка, принимаемого автоматом.
